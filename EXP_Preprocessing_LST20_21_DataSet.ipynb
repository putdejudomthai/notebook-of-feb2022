{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EXP_Preprocessing_LST20-21_DataSet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Pre Download and Process LST20-21 Dataset\n",
        "### Size 53M Words"
      ],
      "metadata": {
        "id": "cEgZ4hBYEYVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "t5DdhtUOJ_WW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1Arufo_uubWbHBvCG7M7r6JvD7zxhP5OV"
      ],
      "metadata": {
        "id": "06Td5Z3aCAZt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2506dd4a-fa3c-4c23-abed-465f138cbe97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Arufo_uubWbHBvCG7M7r6JvD7zxhP5OV\n",
            "To: /content/LST20-21.zip\n",
            "100% 345M/345M [00:02<00:00, 119MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip LST20-21.zip"
      ],
      "metadata": {
        "id": "nh5PmMu5Hb1l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "596ed322-724d-43aa-9b9a-ff8ff3b146f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  LST20-21.zip\n",
            "replace LST20-21/Annotation Guidelines/LST20 Annotation Guideline.pdf? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd LST20-21\n",
        "!unzip -q LST20.zip\n",
        "!unzip -q LST21.zip\n",
        "!ls\n",
        "!rm -r LST20/test/PaxHeader\n",
        "%cd .."
      ],
      "metadata": {
        "id": "BHf9xgM6Hyo1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd28ad34-3dbf-406a-c110-50f70cefe930"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LST20-21\n",
            "replace LST20/DESCRIPTION.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n",
            "unzip:  cannot find or open LST21.zipN, LST21.zipN.zip or LST21.zipN.ZIP.\n",
            "'Annotation Guidelines'   LST20   LST20.zip   LST21   LST21.zip\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#data_path = \"/content/LST20-21/LST20/\" #แก้ path ด้วยยยยยยยยย\n",
        "data_path = \"/content/LST20-21/\"\n",
        "path_lst20 = \"LST20/\"\n",
        "path_lst21 = \"LST21/\"\n",
        "def createCSVSentenceRow(setName): # parameter คือ ชื่อชุดข้อมูล train,eval,test\n",
        "  word=[] \n",
        "  POStag=[]\n",
        "  NERtag=[]\n",
        "  clauseBound=[]\n",
        "  sentenceBound=[]\n",
        "\n",
        "  path = data_path + path_lst20 + setName # path ไปยัง folder train,test หรือ eval\n",
        "  fileNameList = os.listdir(path) # list file ทั้งหมด ใน path ออกมา\n",
        "  \n",
        "  for file_idx in tqdm(range(len(fileNameList))): # list file ทั้งหมด ใน path\n",
        "\n",
        "    tmpWord, tmpPOS, tmpNER, tmpClauseBound, tmpSentenceBound = [],[],[],[],[]\n",
        "\n",
        "    fileName=fileNameList[file_idx]\n",
        "    if fileName[0]!=\".\":\n",
        "\n",
        "      f=open(path+\"/\"+fileName)\n",
        "      lines=f.readlines()\n",
        "      \n",
        "      count = 0\n",
        "      for line in lines:\n",
        "        #print(str(i) +\" ) \"+ line)\n",
        "        if line == \"\\n\":  # ถ้าเป็นบรรทัดว่าง = ตัวแบ่งประโยค = row ก่อนหน้า คือ ตัวสิ้นสุดประโยค = add row เข้า dataframe\n",
        "          count += 1\n",
        "          #print(path + fileName + \"/\" + fileDirList[itr], end = '')\n",
        "          if count <= 1:\n",
        "            tmpSentenceBound[-1] = \"E_SENT\"\n",
        "            tmpSentenceBound.append(\"O\")\n",
        "            word.append(tmpWord)\n",
        "            POStag.append(tmpPOS)\n",
        "            NERtag.append(tmpNER)\n",
        "            clauseBound.append(tmpClauseBound)\n",
        "            sentenceBound.append(tmpSentenceBound)\n",
        "            tmpWord,tmpPOS,tmpNER,tmpClauseBound,tmpSentenceBound = [],[],[],[],[]\n",
        "          continue\n",
        "        else:\n",
        "          count = 0\n",
        "          tmpLine = line.split(\"\\n\")[0].split(\"\\t\")\n",
        "          tmpWord.append(tmpLine[0])\n",
        "          tmpPOS.append(tmpLine[1])\n",
        "          tmpNER.append(tmpLine[2])\n",
        "          tmpClauseBound.append(tmpLine[3])\n",
        "        \n",
        "        if len(tmpSentenceBound) > 0:\n",
        "          tmpSentenceBound.append(\"I_SENT\")\n",
        "        else :\n",
        "          tmpSentenceBound.append(\"B_SENT\")\n",
        "\n",
        "  df = pd.DataFrame(list(zip(word,POStag,NERtag,clauseBound,sentenceBound)), \\\n",
        "                    columns=[\"word\",\"POStag\",\"NERtag\",\"clauseBound\",\"sentenceBound\"])\n",
        "  return df\n",
        "\n",
        "train_df = createCSVSentenceRow(\"train\")\n",
        "eval_df = createCSVSentenceRow(\"eval\")\n",
        "test_df = createCSVSentenceRow(\"test\")"
      ],
      "metadata": {
        "id": "rZTP_DNRL75x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71be98f9-8a33-468a-851f-7c7bedc1ba97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3794/3794 [00:09<00:00, 409.82it/s]\n",
            "100%|██████████| 474/474 [00:00<00:00, 1098.27it/s]\n",
            "100%|██████████| 966/966 [00:00<00:00, 1954.53it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = data_path + path_lst21 # path ไปยัง folder train,test หรือ eval\n",
        "fileNameList = os.listdir(path)\n",
        "#print(fileNameList)\n",
        "\n",
        "word=[] \n",
        "POStag=[]\n",
        "NERtag=[]\n",
        "clauseBound=[]\n",
        "sentenceBound=[]\n",
        "\n",
        "for i in tqdm(range(len(fileNameList))):\n",
        "  tmpWord, tmpPOS, tmpNER, tmpClauseBound, tmpSentenceBound = [],[],[],[],[]\n",
        "  fileName=fileNameList[i]\n",
        "\n",
        "  if fileName[0]!=\".\" and fileName[0] == 'm' and fileName[8] == '_':\n",
        "    fileDirList = os.listdir(path + fileName)\n",
        "\n",
        "    for itr in range(len(fileDirList)):\n",
        "      f=open(path + fileName + \"/\" + fileDirList[itr])\n",
        "      #print(path + fileName + \"/\" + fileDirList[itr])\n",
        "      lines = f.readlines()\n",
        "      #print('\\r', end ='')\n",
        "      count = 0\n",
        "      for line in lines:\n",
        "        #print(str(i) +\" ) \"+ line)\n",
        "        if line == \"\\n\":  # ถ้าเป็นบรรทัดว่าง = ตัวแบ่งประโยค = row ก่อนหน้า คือ ตัวสิ้นสุดประโยค = add row เข้า dataframe\n",
        "          count += 1\n",
        "          #print(path + fileName + \"/\" + fileDirList[itr], end = '')\n",
        "          if count <= 1:\n",
        "            tmpSentenceBound[-1] = \"E_SENT\"\n",
        "            tmpSentenceBound.append(\"O\")\n",
        "            word.append(tmpWord)\n",
        "            POStag.append(tmpPOS)\n",
        "            NERtag.append(tmpNER)\n",
        "            clauseBound.append(tmpClauseBound)\n",
        "            sentenceBound.append(tmpSentenceBound)\n",
        "            tmpWord,tmpPOS,tmpNER,tmpClauseBound,tmpSentenceBound = [],[],[],[],[]\n",
        "          continue\n",
        "        else:\n",
        "          count = 0\n",
        "          tmpLine = line.split(\"\\n\")[0].split(\"\\t\")\n",
        "          tmpWord.append(tmpLine[0])\n",
        "          tmpPOS.append(tmpLine[1])\n",
        "          tmpNER.append(tmpLine[2])\n",
        "          tmpClauseBound.append(tmpLine[3])\n",
        "        \n",
        "        if len(tmpSentenceBound) > 0:\n",
        "          tmpSentenceBound.append(\"I_SENT\")\n",
        "        else :\n",
        "          tmpSentenceBound.append(\"B_SENT\")\n",
        "df_lst21 = pd.DataFrame(list(zip(word,POStag,NERtag,clauseBound,sentenceBound)), \\\n",
        "                    columns=[\"word\",\"POStag\",\"NERtag\",\"clauseBound\",\"sentenceBound\"])"
      ],
      "metadata": {
        "id": "hASe_afzzG4A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22ae94d8-46da-4252-e2ac-5e44b02ffdfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [02:12<00:00,  9.46s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_lst21[[\"word\",\"sentenceBound\"]].tail"
      ],
      "metadata": {
        "id": "nBLTqiFIFuih",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45d0f2a6-a36d-4e30-ae4f-737f4a3a385c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.tail of                                                      word                                      sentenceBound\n",
              "0                                        [ญาณ, สื่อ, รัก]                           [B_SEN, I_SEN, E_SEN, O]\n",
              "1       [ญาณ, สื่อ, รัก, _, เป็น, หนังสือ, นิยาย, เล่ม...  [B_SEN, I_SEN, I_SEN, I_SEN, I_SEN, I_SEN, I_S...\n",
              "2       [ญาณิน, _, ผู้, มี, ญาณ, วิเศษ, สามารถ, สื่อสา...  [B_SEN, I_SEN, I_SEN, I_SEN, I_SEN, I_SEN, I_S...\n",
              "3                            [[, [, หมวดหมู่, :, นวนิยาย]             [B_SEN, I_SEN, I_SEN, I_SEN, E_SEN, O]\n",
              "4       [[, [, หมวดหมู่, :, วรรณกรรม, ใน, ปี, _, พ.ศ.,...  [B_SEN, I_SEN, I_SEN, I_SEN, I_SEN, I_SEN, I_S...\n",
              "...                                                   ...                                                ...\n",
              "690245  [หวีด, สุด, ขีด, _, (, ), ภาพยนตร์, แนว, ฆาตกร...  [B_SEN, I_SEN, I_SEN, I_SEN, I_SEN, I_SEN, I_S...\n",
              "690246  [หลัง, จาก, การ, เสีย, ชีวิต, ของ, แม่, ซิดนีย...  [B_SEN, I_SEN, I_SEN, I_SEN, I_SEN, I_SEN, I_S...\n",
              "690247  [\", Scream, \", _, เป็น, ภาพยนตร์, ฆาตกรรม, ต่อ...  [B_SEN, I_SEN, I_SEN, I_SEN, I_SEN, I_SEN, I_S...\n",
              "690248  [ด, รูว์, _, แบร์รี, มอร์, _, นัก, แสดง, หญิง,...  [B_SEN, I_SEN, I_SEN, I_SEN, I_SEN, I_SEN, I_S...\n",
              "690249  [จาก, ความ, สำเร็จ, ของ, _, \", Scream, \", _, ท...  [B_SEN, I_SEN, I_SEN, I_SEN, I_SEN, I_SEN, I_S...\n",
              "\n",
              "[690250 rows x 2 columns]>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_lst21, test_lst21 = train_test_split(df_lst21, test_size=0.2)\n",
        "train_lst21, val_lst21 = train_test_split(train_lst21, test_size=0.2)"
      ],
      "metadata": {
        "id": "Rtj49OOZ37q7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_lst21.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXkJvN-B98ck",
        "outputId": "6aea60b9-a84d-4f8b-bd70-fa267feab05e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(441760, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_df.append(train_lst21,ignore_index=True)\n",
        "eval_df = eval_df.append(val_lst21,ignore_index=True)\n",
        "test_df = test_df.append(test_lst21,ignore_index=True)"
      ],
      "metadata": {
        "id": "LstgBDXmBdMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5alStL2R9W6d",
        "outputId": "bf990c63-f74a-4d78-aed4-586a5d803910"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(505070, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    }
  ]
}